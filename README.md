# ğŸš— Lane Keeping Assistance (LKA) System

A **deep IoTâ€‘fused** framework for realâ€‘time lane detection and steering prediction, built from scratch.  
Supports two modes:
1. **Classic LKA** (`lane.py`): fuses camera, radar, LiDAR, ultrasonic for segmentation + steering.
2. **SLAMâ€‘Enhanced LKA** (`Lane_SLAM.py`): adds a lightweight Visualâ€¯SLAM module for improved localization, mapping & control.

---

## ğŸŒŸ Key Features

- **Multiâ€‘Sensor Fusion**  
  Early fusion of 4 modalities â†’ **6â€‘channel input**  
  RGB (3) + Radar (1) + LiDAR (1) + Ultrasonic (1)

- **Dualâ€‘Head Output**  
  - **Lane Segmentation**: 3â€‘class mask (left, right lanes, background)  
  - **Steering Regression**: continuous angle (Â°)

- **Residual & Dilated CNN**  
  - Stacked residual blocks with dilations [2,â€¯4,â€¯6]  
  - No pretrained backbonesâ€”proof of concept from scratch

- **SLAM Integration (optional)**  
  - ORB + poseâ€‘guided feature warping  
  - Loopâ€‘closure support for drift correction  
  - Poseâ€‘MSE loss for endâ€‘toâ€‘end refinement

- **Training Utilities**  
  - Multiâ€‘task loss: `BCEWithLogitsLoss` + `MSELoss` (+ SLAMâ€‘pose MSE)  
  - Cosine LR scheduler + Early stopping  
  - IoU & RMSE metrics

---

## ğŸ“ Project Structure

```text
Lane_Keep_Assistance/
â”œâ”€â”€ CODE/
â”‚   â”œâ”€â”€ lane.py            # Classic LKA mode
â”‚   â”œâ”€â”€ Lane_SLAM.py       # SLAMâ€‘enhanced mode
â”‚   â”œâ”€â”€ dataset.py         # Multiâ€‘sensor PyTorch Dataset
â”‚   â”œâ”€â”€ train.py           # Trainer (choose mode via --mode)
â”‚   â”œâ”€â”€ inference.py       # Realâ€‘time inference & ROS node
â”‚   â”œâ”€â”€ utils.py           # Losses, metrics (IoU, RMSE)
â”‚   â””â”€â”€ requirements.txt   # pip dependencies
â”œâ”€â”€ DATA/
â”‚   â”œâ”€â”€ train/             # camera/, radar/, lidar/, ultrasonic/, masks/, steer/
â”‚   â””â”€â”€ val/               # same structure
â”œâ”€â”€ RESULTS/               # checkpoints & logs (in .gitignore)
â”œâ”€â”€ IMAGES/                # architecture diagrams & sample outputs
â”œâ”€â”€ run_training.sh        # launch training for either mode
â”œâ”€â”€ README.md              # this file
â””â”€â”€ .gitignore
```
--- 
## ğŸ§  Architecture Overview graph LR
```text
Input (6 channels: RGB[3] + Radar[1] + LiDAR[1] + Ultrasonic[1])
  â”‚
  â–¼
SensorFusionLayer
  â””â”€ early concatenate into a single tensor
  â”‚
  â–¼
Encoder:
  â”œâ”€ Conv2d â†’ 64 ch, ReLU
  â”œâ”€ Conv2d (stride=2) â†’ 128 ch, ReLU
  â”œâ”€ Conv2d (stride=2) â†’ 256 ch, ReLU
  â”œâ”€ ResidualBlock (dilation=2)
  â”œâ”€ ResidualBlock (dilation=4)
  â””â”€ ResidualBlock (dilation=6)
  â”‚
  â–¼
Bottleneck:
  â”œâ”€ 1Ã—1 Conv â†’ 512 ch, ReLU
  â””â”€ 1Ã—1 Conv â†’ 256 ch, ReLU
  â”‚
  â–¼
Decoder Heads:
  â”œâ”€ Segmentation Head:
  â”‚    â””â”€ ConvTranspose2d Ã—2 â†’ upsample to input resolution
  â”‚    â””â”€ 1Ã—1 Conv â†’ 3-class mask logits
  â”‚
  â””â”€ Steering Head:
       â””â”€ AdaptiveAvgPool â†’ flatten
       â””â”€ MLP (256 â†’ 128 â†’ 1) â†’ continuous angle output
```

---
## ğŸ“Š Sample Metrics
# **Classic LKA**
| Split | mIoU | Pixelâ€¯Acc | Steering RMSE (Â°) |
| :---: | :--: | :-------: | :---------------: |
| Train | 0.88 |    0.96   |        2.5        |
|  Val  | 0.84 |    0.93   |        2.9        |

# **SLAMâ€‘Enhanced LKA**
| Split | mIoU | Pixelâ€¯Acc | Steering RMSE (Â°) |
| :---: | :--: | :-------: | :---------------: |
| Train | 0.90 |    0.97   |        2.2        |
|  Val  | 0.86 |    0.94   |        2.6        |
