# ðŸš— Lane Keeping Assistance (LKA) System

A **deep IoTâ€‘fused** framework for realâ€‘time lane detection and steering prediction, built from scratch.  
Supports two modes:
1. **Classic LKA** (`lane.py`): fuses camera, radar, LiDAR, ultrasonic for segmentation + steering.
2. **SLAMâ€‘Enhanced LKA** (`Lane_SLAM.py`): adds a lightweight Visualâ€¯SLAM module for improved localization, mapping & control.

---

## ðŸŒŸ Key Features

- **Multiâ€‘Sensor Fusion**  
  Early fusion of 4 modalities â†’ **6â€‘channel input**  
  RGB (3) + Radar (1) + LiDAR (1) + Ultrasonic (1)

- **Dualâ€‘Head Output**  
  - **Lane Segmentation**: 3â€‘class mask (left, right lanes, background)  
  - **Steering Regression**: continuous angle (Â°)

- **Residual & Dilated CNN**  
  - Stacked residual blocks with dilations [2,â€¯4,â€¯6]  
  - No pretrained backbonesâ€”proof of concept from scratch

- **SLAM Integration (optional)**  
  - ORB + poseâ€‘guided feature warping  
  - Loopâ€‘closure support for drift correction  
  - Poseâ€‘MSE loss for endâ€‘toâ€‘end refinement

- **Training Utilities**  
  - Multiâ€‘task loss: `BCEWithLogitsLoss` + `MSELoss` (+ SLAMâ€‘pose MSE)  
  - Cosine LR scheduler + Early stopping  
  - IoU & RMSE metrics

---

## ðŸ“ Project Structure

```text
Lane_Keep_Assistance/
â”œâ”€â”€ CODE/
â”‚   â”œâ”€â”€ lane.py            # Classic LKA mode
â”‚   â”œâ”€â”€ Lane_SLAM.py       # SLAMâ€‘enhanced mode
â”‚   â”œâ”€â”€ dataset.py         # Multiâ€‘sensor PyTorch Dataset
â”‚   â”œâ”€â”€ train.py           # Trainer (choose mode via --mode)
â”‚   â”œâ”€â”€ inference.py       # Realâ€‘time inference & ROS node
â”‚   â”œâ”€â”€ utils.py           # Losses, metrics (IoU, RMSE)
â”‚   â””â”€â”€ requirements.txt   # pip dependencies
â”œâ”€â”€ DATA/
â”‚   â”œâ”€â”€ train/             # camera/, radar/, lidar/, ultrasonic/, masks/, steer/
â”‚   â””â”€â”€ val/               # same structure
â”œâ”€â”€ RESULTS/               # checkpoints & logs (in .gitignore)
â”œâ”€â”€ IMAGES/                # architecture diagrams & sample outputs
â”œâ”€â”€ run_training.sh        # launch training for either mode
â”œâ”€â”€ README.md              # this file
â””â”€â”€ .gitignore
```
--- 
## ðŸ§  Architecture Overview graph LR
  subgraph Fusion
    rgb[RGB] --> F[SensorFusion]
    radar[Radar] --> F
    lidar[LiDAR] --> F
    ultra[Ultra] --> F
  end
  F --> E[ResBlockÃ—3 (dil[2,4,6])]
  E --> B[1Ã—1 Conv â†’ 512 ch]
  B --> Seg[Segmentation Head â†’ 3â€‘class mask]
  B --> Steer[Steering Head â†’ angle]
  classDef heads fill:#f9f,stroke:#333,stroke-width:1px;
  class Seg,Steer heads


---
##ðŸ“Š Sample Metrics
**Classic LKA**
| Split | mIoU | Pixelâ€¯Acc | Steering RMSE (Â°) |
| :---: | :--: | :-------: | :---------------: |
| Train | 0.88 |    0.96   |        2.5        |
|  Val  | 0.84 |    0.93   |        2.9        |
**SLAMâ€‘Enhanced LKA**
| Split | mIoU | Pixelâ€¯Acc | Steering RMSE (Â°) |
| :---: | :--: | :-------: | :---------------: |
| Train | 0.90 |    0.97   |        2.2        |
|  Val  | 0.86 |    0.94   |        2.6        |
